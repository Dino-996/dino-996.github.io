<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>dino-996 blog</title>
  <subtitle>Developer portfolio e blog tecnico. Condivido articoli su programmazione, web development e tecnologie moderne.</subtitle>
  <link href="https://dino-996.github.io/feed.xml" rel="self" type="application/atom+xml"/>
  <link href="https://dino-996.github.io" rel="alternate" type="text/html"/>
  <id>https://dino-996.github.io/</id>
  <updated>2020-02-21T00:00:00.000Z</updated>
  <author>
    <name>Davide Sabia</name>
    <email>davidesabia22@gmail.com</email>
    <uri>https://dino-996.github.io</uri>
  </author>

  
  <entry>
    <title>Scheduling della CPU</title>
    <link href="https://dino-996.github.io/blog/scheduling-della-cpu/" rel="alternate" type="text/html"/>
    <id>https://dino-996.github.io/blog/scheduling-della-cpu/</id>
    <published>2020-02-21T00:00:00.000Z</published>
    <updated>2020-02-21T00:00:00.000Z</updated>
    
    <summary>Facciamo un&#39;analisi approfondita dello scheduling della CPU, una funzione vitale dei sistemi operativi progettata per massimizzare l&#39;efficienza attraverso la multiprogrammazione.</summary>
    
    
    <content type="html"><![CDATA[&lt;h1&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;In questo articolo esploriamo lo scheduling della CPU, una funzione vitale dei sistemi operativi progettata per ottimizza throughput e tempi d’attesa attraverso algoritmi come FCFS, SJF e Round Robin. La gestione moderna si estende a thread hardware/software e sistemi multicore, privilegiando la processor affinity per l&#39;efficienza della cache. Il supporto real-time (soft/hard) garantisce il rispetto di scadenze rigide.&lt;/p&gt;
&lt;h2&gt;Cos’è lo scheduling della CPU&lt;/h2&gt;
&lt;p&gt;Lo &lt;strong&gt;scheduling della CPU&lt;/strong&gt; è il meccanismo attraverso il quale il sistema operativo decide quale processo o thread debba essere eseguito in un determinato istante. Poiché un singolo core può eseguire un solo flusso di esecuzione alla volta, lo scheduler ha il compito di distribuire il tempo di CPU tra più entità concorrenti, garantendo efficienza, equità e reattività.&lt;/p&gt;
&lt;p&gt;Nei sistemi moderni lo scheduler è una componente critica del kernel: deve operare con latenza minima, comportamento prevedibile e capacità di scalare su architetture multicore o multiprocessore.&lt;/p&gt;
&lt;h2&gt;Concetti fondamentali&lt;/h2&gt;
&lt;h3&gt;CPU burst e I/O burst&lt;/h3&gt;
&lt;p&gt;L’esecuzione di un processo è caratterizzata dall’alternanza tra:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CPU burst&lt;/strong&gt;, fasi di elaborazione attiva;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;I/O burst&lt;/strong&gt;, fasi di attesa per operazioni di input/output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questa struttura ciclica costituisce il fondamento della multiprogrammazione: mentre un processo è bloccato in attesa di I/O, un altro può utilizzare la CPU, aumentando l’utilizzo complessivo del sistema.&lt;/p&gt;
&lt;h3&gt;Modalità di prelazione (preemption)&lt;/h3&gt;
&lt;p&gt;Si distinguono due modelli principali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Non preemptive&lt;/strong&gt;: il processo mantiene la CPU fino al completamento o a un evento di blocco (ad esempio una richiesta di I/O).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Preemptive&lt;/strong&gt;: lo scheduler può interrompere un processo attivo, tipicamente tramite interrupt generati da un timer hardware, per assegnare la CPU a un altro processo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La preemption migliora la reattività nei sistemi interattivi, ma richiede meccanismi di sincronizzazione accurati per preservare la coerenza dello stato del kernel.&lt;/p&gt;
&lt;h3&gt;Dispatcher e context switch&lt;/h3&gt;
&lt;p&gt;Il &lt;strong&gt;dispatcher&lt;/strong&gt; è il modulo responsabile dell’effettivo trasferimento del controllo della CPU al processo selezionato. Le sue operazioni includono:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;salvataggio e ripristino dei registri (context switch);&lt;/li&gt;
&lt;li&gt;aggiornamento delle strutture di controllo del kernel;&lt;/li&gt;
&lt;li&gt;passaggio dalla modalità kernel alla modalità utente.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Il tempo necessario a eseguire queste operazioni è detto &lt;strong&gt;latenza di dispatch&lt;/strong&gt; e rappresenta un overhead che deve essere contenuto per mantenere elevate prestazioni.&lt;/p&gt;
&lt;h2&gt;Criteri di valutazione&lt;/h2&gt;
&lt;p&gt;Gli algoritmi di scheduling sono confrontati mediante metriche quantitative standard:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Utilizzo della CPU&lt;/strong&gt;: percentuale di tempo in cui la CPU esegue lavoro utile.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Throughput&lt;/strong&gt;: numero di processi completati per unità di tempo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Turnaround time&lt;/strong&gt;: intervallo tra l’arrivo del processo nel sistema e la sua conclusione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tempo di attesa&lt;/strong&gt;: tempo totale trascorso nella ready queue.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tempo di risposta&lt;/strong&gt;: intervallo tra la richiesta di esecuzione e la prima risposta prodotta, rilevante nei sistemi interattivi.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Queste metriche possono entrare in conflitto: ad esempio, la riduzione del tempo di risposta può aumentare il numero di context switch e quindi l’overhead complessivo.&lt;/p&gt;
&lt;h2&gt;Algoritmi di scheduling&lt;/h2&gt;
&lt;h3&gt;FCFS (First-Come, First-Served)&lt;/h3&gt;
&lt;p&gt;Il processo con ordine di arrivo più basso nella ready queue viene eseguito per primo. È un algoritmo semplice e non preemptive, ma può generare il &lt;strong&gt;convoy effect&lt;/strong&gt;, in cui un processo lungo ritarda l’esecuzione di numerosi processi brevi.&lt;/p&gt;
&lt;h3&gt;SJF (Shortest-Job-First) e SRTF&lt;/h3&gt;
&lt;p&gt;L’algoritmo &lt;strong&gt;SJF&lt;/strong&gt; seleziona il processo con il CPU burst stimato più breve, minimizzando il tempo di attesa medio in condizioni ideali. La variante preemptive è detta &lt;strong&gt;Shortest Remaining Time First (SRTF)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;La principale criticità consiste nella stima della durata futura dei burst, generalmente ottenuta tramite tecniche di predizione statistica (ad esempio media esponenziale).&lt;/p&gt;
&lt;h3&gt;Round Robin (RR)&lt;/h3&gt;
&lt;p&gt;Ogni processo riceve un intervallo di tempo prefissato, detto &lt;strong&gt;quantum&lt;/strong&gt;. Allo scadere del quantum, il processo viene sospeso e reinserito nella ready queue.&lt;/p&gt;
&lt;p&gt;La scelta del quantum è determinante:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;un valore troppo elevato rende il comportamento simile a FCFS;&lt;/li&gt;
&lt;li&gt;un valore troppo ridotto aumenta l’overhead dovuto ai frequenti context switch.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Scheduling con priorità&lt;/h3&gt;
&lt;p&gt;A ogni processo è assegnato un livello di priorità; la CPU viene concessa al processo con priorità più alta.&lt;/p&gt;
&lt;p&gt;Questo approccio può causare &lt;strong&gt;starvation&lt;/strong&gt; per i processi a bassa priorità. Il fenomeno è mitigabile mediante &lt;strong&gt;aging&lt;/strong&gt;, ossia l’incremento progressivo della priorità dei processi in attesa.&lt;/p&gt;
&lt;h3&gt;Code multilivello e multilevel feedback queue (MLFQ)&lt;/h3&gt;
&lt;p&gt;Le &lt;strong&gt;multilevel queue&lt;/strong&gt; suddividono i processi in code distinte (ad esempio interattivi, batch, di sistema), ciascuna con una propria politica di scheduling.&lt;/p&gt;
&lt;p&gt;Le &lt;strong&gt;multilevel feedback queue (MLFQ)&lt;/strong&gt; consentono ai processi di spostarsi tra code in base al comportamento osservato, realizzando uno scheduling adattativo che favorisce i processi interattivi senza penalizzare eccessivamente quelli computazionali.&lt;/p&gt;
&lt;h2&gt;Scheduling in sistemi multiprocessore&lt;/h2&gt;
&lt;p&gt;Nei sistemi &lt;strong&gt;SMP (Symmetric Multiprocessing)&lt;/strong&gt;, ogni core può gestire una coda locale oppure condividere una coda globale.&lt;/p&gt;
&lt;p&gt;Concetti rilevanti includono:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Processor affinity&lt;/strong&gt;: mantenere un processo sullo stesso core per sfruttare la località della cache e ridurre i costi di migrazione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load balancing&lt;/strong&gt;: distribuire il carico in modo uniforme tra i core per evitare squilibri.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Sistemi real-time&lt;/h2&gt;
&lt;p&gt;Nei sistemi real-time il rispetto delle deadline costituisce un requisito primario.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Soft real-time&lt;/strong&gt;: il mancato rispetto di una deadline degrada le prestazioni ma non compromette necessariamente il sistema.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hard real-time&lt;/strong&gt;: il mancato rispetto di una deadline è considerato un errore critico.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Algoritmi classici includono:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rate Monotonic Scheduling (RMS)&lt;/strong&gt;: assegna priorità fisse proporzionali alla frequenza dei task periodici.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Earliest Deadline First (EDF)&lt;/strong&gt;: assegna priorità dinamiche in base alla deadline più prossima; è ottimale su singolo processore in condizioni ideali di modello.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Scheduler nei principali sistemi operativi&lt;/h2&gt;
&lt;h3&gt;Linux&lt;/h3&gt;
&lt;p&gt;Linux utilizza più classi di scheduling. Per i task ordinari adotta il &lt;strong&gt;Completely Fair Scheduler (CFS)&lt;/strong&gt;, che modella l’esecuzione come una condivisione equa del tempo di CPU e utilizza strutture dati bilanciate (ad esempio alberi red-black) per selezionare il task con il minor tempo virtuale accumulato.&lt;/p&gt;
&lt;p&gt;Sono inoltre presenti classi dedicate ai task real-time e meccanismi di bilanciamento del carico su sistemi SMP.&lt;/p&gt;
&lt;h3&gt;Windows&lt;/h3&gt;
&lt;p&gt;Windows implementa uno scheduler preemptive basato su priorità, con 32 livelli distinti:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;livelli &lt;strong&gt;variabili&lt;/strong&gt; per applicazioni generiche;&lt;/li&gt;
&lt;li&gt;livelli &lt;strong&gt;real-time&lt;/strong&gt; riservati a task critici.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Le priorità possono essere modificate dinamicamente per migliorare la reattività delle applicazioni interattive e prevenire situazioni di starvation.&lt;/p&gt;
]]></content>
    
    
      
    
      
    <category term="sistemi operativi"/>
      
    
      
    <category term="scheduling"/>
      
    
  </entry>
  
  <entry>
    <title>Thread</title>
    <link href="https://dino-996.github.io/blog/thread/" rel="alternate" type="text/html"/>
    <id>https://dino-996.github.io/blog/thread/</id>
    <published>2020-02-18T00:00:00.000Z</published>
    <updated>2020-02-18T00:00:00.000Z</updated>
    
    <summary>Si descrivono i thread come le unità di base di utilizzo della CPU, evidenziando i vantaggi del multithreading come la condivisione delle risorse e la scalabilità. Approfondiamo il calcolo parallelo confrontando CPU e GPU, citando linguaggi come CUDA e OpenCL per elaborazioni ad alte prestazioni. Analizziamo inoltre l&#39;uso di librerie standard (Pthreads, Windows API, Java Executor) e modelli come thread pool e fork-join, concludendo con le sfide della cancellazione dei task e le specificità di sistema come la funzione clone() in Linux.</summary>
    
    
    <content type="html"><![CDATA[&lt;h1&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;In questo articolo esploriamo l&#39;architettura dei thread e la programmazione concorrente all&#39;interno dei sistemi operativi moderni.&lt;/p&gt;
&lt;h2&gt;Thread&lt;/h2&gt;
&lt;p&gt;Il thread rappresenta l’unità minima di esecuzione schedulabile dalla CPU all’interno di un processo. Dal punto di vista architetturale, ciascun thread è caratterizzato da:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;un identificatore univoco;&lt;/li&gt;
&lt;li&gt;un Program Counter (PC), che individua l’istruzione corrente;&lt;/li&gt;
&lt;li&gt;un insieme di registri di stato;&lt;/li&gt;
&lt;li&gt;uno stack privato per la gestione delle chiamate di funzione e delle variabili locali.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All’interno dello stesso &lt;strong&gt;&lt;a href=&quot;https://dino-996.github.io/blog/processi/&quot;&gt;processo&lt;/a&gt;&lt;/strong&gt;, i thread condividono lo spazio di indirizzamento e le risorse globali, tra cui il segmento di codice, il segmento dati (heap e variabili globali) e le risorse di sistema allocate (file descriptor, segnali, socket). Tale condivisione costituisce l’elemento distintivo rispetto al modello multiprocesso tradizionale, nel quale ogni processo possiede uno spazio di memoria isolato.&lt;/p&gt;
&lt;p&gt;L’introduzione del multithreading risponde a esigenze di efficienza computazionale e di progettazione del software concorrente. I principali benefici possono essere formalizzati come segue:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Riduzione della latenza percepita&lt;/strong&gt;: la suddivisione del lavoro in più thread consente la sovrapposizione tra operazioni di I/O e computazione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficienza nell’uso delle risorse&lt;/strong&gt;: il costo di creazione e distruzione di un thread è inferiore rispetto a quello di un processo, così come l’overhead del context switch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modularità progettuale&lt;/strong&gt;: la decomposizione di un’applicazione in unità concorrenti favorisce una strutturazione più chiara delle responsabilità.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalabilità su architetture multicore&lt;/strong&gt;: la presenza di più unità di esecuzione fisiche permette l’esecuzione realmente parallela dei thread.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Concorrenza, Parallelismo e Architetture Eterogenee&lt;/h2&gt;
&lt;p&gt;Nel contesto dei sistemi moderni è necessario distinguere rigorosamente tra concorrenza e parallelismo, abbiamo già affrontato l&#39;argomento quando parlavamo dei processi ma è bene ripetere i concetti al fine di fissare bene le idee.&lt;/p&gt;
&lt;p&gt;La &lt;strong&gt;concorrenza&lt;/strong&gt; è una proprietà logica del sistema: più task progrediscono nel tempo, anche se non necessariamente in modo simultaneo. In un sistema single-core, ciò avviene tramite time slicing e meccanismi di scheduling preemptive.&lt;/p&gt;
&lt;p&gt;Il &lt;strong&gt;parallelismo&lt;/strong&gt;, invece, è una proprietà fisica dell’hardware: più task sono eseguiti simultaneamente su core distinti. Il parallelismo rappresenta quindi un caso particolare di concorrenza in presenza di risorse computazionali multiple.&lt;/p&gt;
&lt;p&gt;L’evoluzione delle architetture ha introdotto sistemi eterogenei CPU–GPU. Le GPU (Graphics Processing Unit), inizialmente progettate per il rendering grafico, sono oggi impiegate nel paradigma GPGPU (General-Purpose computing on GPU). Tali dispositivi integrano un numero elevato di ALU (Arithmetic Logic Units), ottimizzate per il calcolo massivamente parallelo su grandi insiemi di dati omogenei.&lt;/p&gt;
&lt;p&gt;Applicazioni tipiche includono:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;calcolo scientifico e bioinformatica;&lt;/li&gt;
&lt;li&gt;crittografia e analisi numerica;&lt;/li&gt;
&lt;li&gt;addestramento di modelli di deep learning;&lt;/li&gt;
&lt;li&gt;simulazioni fisiche su larga scala.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Modelli di Implementazione del Multithreading&lt;/h2&gt;
&lt;p&gt;Dal punto di vista sistemistico, si distinguono thread a livello utente e thread a livello kernel.&lt;/p&gt;
&lt;p&gt;I &lt;strong&gt;thread a livello utente&lt;/strong&gt; sono gestiti da librerie in spazio utente, senza intervento diretto del kernel nelle operazioni di scheduling interne al processo.&lt;/p&gt;
&lt;p&gt;I &lt;strong&gt;thread a livello kernel&lt;/strong&gt; sono invece entità direttamente note al sistema operativo, il quale ne gestisce pianificazione, sospensione e sincronizzazione.&lt;/p&gt;
&lt;p&gt;La relazione tra thread utente e thread kernel è formalizzata attraverso modelli di mapping:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Many-to-One&lt;/strong&gt;: più thread utente sono associati a un singolo thread kernel. Il modello minimizza l’overhead (indica il costo aggiuntivo necessario per gestire un’operazione, costo che non contribuisce direttamente al risultato utile, ma è indispensabile per renderla possibile), ma una chiamata bloccante compromette l’intero processo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-to-One&lt;/strong&gt;: ogni thread utente corrisponde a un thread kernel. Garantisce parallelismo reale su sistemi multicore, al prezzo di un maggiore consumo di risorse.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Many-to-Many&lt;/strong&gt;: più thread utente sono mappati su un numero limitato di thread kernel, consentendo un compromesso tra flessibilità e scalabilità.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two-Level Model&lt;/strong&gt;: estensione del many-to-many che consente l’associazione vincolata di specifici thread utente a determinati thread kernel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La scelta del modello influenza direttamente prestazioni, prevedibilità temporale e complessità di gestione.&lt;/p&gt;
&lt;h2&gt;Astrazioni di Programmazione e Threading Implicito&lt;/h2&gt;
&lt;p&gt;Le principali interfacce di programmazione concorrente includono:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;POSIX Threads (Pthreads)&lt;/strong&gt;: standard che definisce primitive per creazione, sincronizzazione e gestione dei thread in ambienti Unix-like.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;API Windows&lt;/strong&gt;: insieme di primitive integrate nel kernel NT per la gestione nativa dei thread.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Java Concurrency Framework&lt;/strong&gt;: insieme di astrazioni ad alto livello (Executor, ExecutorService, Callable, Future) che separano la definizione del task dalla gestione dei thread sottostanti.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Il paradigma del &lt;strong&gt;threading implicito&lt;/strong&gt; trasferisce la responsabilità della gestione concorrente a runtime e librerie specializzate. Tra i principali strumenti:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Thread Pool&lt;/strong&gt;: insieme di thread riutilizzabili per l’esecuzione di task asincroni, con controllo del grado di parallelismo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fork/Join Framework&lt;/strong&gt;: modello ricorsivo divide-et-impera basato su work stealing per l’ottimizzazione del bilanciamento del carico.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenMP, Grand Central Dispatch, Intel TBB&lt;/strong&gt;: tecnologie che forniscono direttive e costrutti di alto livello per la parallelizzazione controllata.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tali astrazioni riducono la probabilità di errori sistemici e favoriscono una gestione più robusta delle risorse concorrenti.&lt;/p&gt;
&lt;h2&gt;Criticità e Differenze tra Sistemi Operativi&lt;/h2&gt;
&lt;p&gt;La programmazione multithread introduce problematiche complesse, tra cui:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;condizioni di race e necessità di meccanismi di sincronizzazione (mutex, semafori, monitor);&lt;/li&gt;
&lt;li&gt;gestione coerente dei segnali in presenza di più flussi di esecuzione;&lt;/li&gt;
&lt;li&gt;utilizzo del Thread-Local Storage (TLS) per mantenere dati isolati per thread;&lt;/li&gt;
&lt;li&gt;politiche di cancellazione e terminazione.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La cancellazione può essere asincrona, con interruzione immediata del thread, oppure differita, mediante punti di cancellazione espliciti. L’approccio differito è generalmente preferito per preservare la consistenza dello stato condiviso.&lt;/p&gt;
&lt;p&gt;Dal punto di vista implementativo emergono differenze significative:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; utilizza strutture interne quali ETHREAD, KTHREAD e TEB (Thread Environment Block) per rappresentare e gestire i thread.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; adotta un modello unificato in cui processi e thread sono rappresentati come task. La system call &lt;code&gt;clone()&lt;/code&gt; consente di specificare, tramite flag, il livello di condivisione delle risorse tra entità padre e figlio, realizzando un modello flessibile di creazione concorrente.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;L’analisi comparativa evidenzia come le scelte progettuali a livello di kernel influenzino direttamente il modello di programmazione esposto agli sviluppatori e le garanzie offerte in termini di isolamento, prestazioni e controllo della concorrenza.&lt;/p&gt;
]]></content>
    
    
      
    
      
    <category term="sistemi operativi"/>
      
    
      
    <category term="thread"/>
      
    
  </entry>
  
  <entry>
    <title>Struttura dei Sistemi Operativi</title>
    <link href="https://dino-996.github.io/blog/struttura-dei-sistemi-operativi/" rel="alternate" type="text/html"/>
    <id>https://dino-996.github.io/blog/struttura-dei-sistemi-operativi/</id>
    <published>2020-02-14T00:00:00.000Z</published>
    <updated>2020-02-14T00:00:00.000Z</updated>
    
    <summary>Si descrivono le diverse modalità di accesso, dalle interfacce testuali (shell) a quelle grafiche touch-screen, evidenziando come le chiamate di sistema permettono ai programmi di comunicare con il kernel.</summary>
    
    
    <content type="html"><![CDATA[&lt;h1&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;In questo articolo vediamo l&#39;architettura dei sistemi operativi, analizzando il modo in cui gestiscono le risorse hardware e interagiscono con l&#39;utente.&lt;/p&gt;
&lt;h2&gt;Servizi e Interfacce del Sistema Operativo&lt;/h2&gt;
&lt;p&gt;Il sistema operativo (&lt;strong&gt;SO&lt;/strong&gt;) fornisce un ambiente per l&#39;esecuzione dei programmi, offrendo servizi essenziali quali l&#39;&lt;strong&gt;esecuzione di programmi&lt;/strong&gt;, operazioni di &lt;strong&gt;I/O&lt;/strong&gt;, gestione del &lt;strong&gt;file system&lt;/strong&gt;, comunicazioni, rilevamento di errori e allocazione delle risorse.&lt;/p&gt;
&lt;p&gt;L&#39;interazione con l&#39;utente avviene attraverso tre modalità principali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interfaccia a riga di comando (CLI):&lt;/strong&gt; utilizza interpreti speciali chiamati &lt;strong&gt;shell&lt;/strong&gt; (come Bash nei sistemi Unix/Linux). È preferita dagli amministratori per l&#39;efficienza e la programmabilità.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interfaccia grafica (GUI):&lt;/strong&gt; basata sulla metafora del desktop, icone e puntatori.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interfaccia touch screen:&lt;/strong&gt; basata su gesti (gesture) direttamente sullo schermo.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Interfaccia di Programmazione - System Call e API&lt;/h2&gt;
&lt;p&gt;Le &lt;strong&gt;chiamate di sistema (system call)&lt;/strong&gt; rappresentano l&#39;interfaccia verso i servizi resi disponibili dal SO.&lt;br&gt;
Spesso i programmatori non invocano direttamente le system call, ma utilizzano le &lt;strong&gt;Application Programming Interface (API)&lt;/strong&gt;, come le API di Windows, POSIX (per Unix, Linux, macOS) o le API Java. Questo garantisce la &lt;strong&gt;portabilità delle applicazioni&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Per passare i parametri al sistema operativo durante una chiamata, si utilizzano tre metodi principali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attraverso i &lt;strong&gt;registri&lt;/strong&gt; della CPU (metodo più veloce).&lt;/li&gt;
&lt;li&gt;In un &lt;strong&gt;blocco o tabella di memoria&lt;/strong&gt;, passando l&#39;indirizzo del blocco in un registro.&lt;/li&gt;
&lt;li&gt;Nello &lt;strong&gt;stack&lt;/strong&gt;, dove i parametri vengono inseriti (&lt;strong&gt;push&lt;/strong&gt;) e poi prelevati dal SO (&lt;strong&gt;pop&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Sviluppo ed Esecuzione delle Applicazioni&lt;/h2&gt;
&lt;p&gt;Il processo che porta un codice sorgente alla sua esecuzione prevede due figure chiave:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linker:&lt;/strong&gt; combina file oggetto rilocabili in un unico &lt;strong&gt;file binario&lt;/strong&gt; eseguibile, includendo eventuali librerie (es. la libreria standard C).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loader:&lt;/strong&gt; carica il file eseguibile in memoria, assegnando gli indirizzi definitivi (&lt;strong&gt;relocation&lt;/strong&gt;) per permettere l&#39;esecuzione sul core della CPU.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Le applicazioni sono generalmente dipendenti dal SO, poiché ogni sistema fornisce un insieme univoco di chiamate di sistema.&lt;/p&gt;
&lt;h2&gt;Kernel&lt;/h2&gt;
&lt;p&gt;Il kernel è responsabile della gestione delle risorse hardware e dei servizi di base del sistema. Le sue attività principali includono:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inizializzazione dell&#39;hardware:&lt;/strong&gt; durante il processo di avvio (&lt;strong&gt;boot&lt;/strong&gt;), il kernel viene caricato in memoria e prepara i componenti fisici del computer per l&#39;uso.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gestione dei processi e della CPU:&lt;/strong&gt; si occupa dello &lt;strong&gt;scheduling&lt;/strong&gt;, decidendo quali programmi devono essere eseguiti dal processore e per quanto tempo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gestione della memoria:&lt;/strong&gt; controlla la memoria fisica, la &lt;strong&gt;memoria virtuale&lt;/strong&gt; e la paginazione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gestione dei file e dei dispositivi:&lt;/strong&gt; amministra il &lt;strong&gt;file system&lt;/strong&gt; (montando il root file system all&#39;avvio) e i driver necessari per interagire con periferiche, dischi e reti.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interfaccia per le chiamate di sistema:&lt;/strong&gt; fornisce un&#39;interfaccia protetta tramite la quale le applicazioni possono richiedere servizi al sistema operativo usando le &lt;strong&gt;system call&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Modalità di Esecuzione&lt;/h3&gt;
&lt;p&gt;Il kernel opera in uno spazio di memoria protetto chiamato &lt;strong&gt;kernel space&lt;/strong&gt; (o modalità kernel), distinto dallo spazio utente dove girano le normali applicazioni.&lt;br&gt;
Questa separazione garantisce la stabilità del sistema: un errore critico all&#39;interno del kernel provoca un &lt;strong&gt;crash di sistema&lt;/strong&gt;, non un semplice bug applicativo.&lt;/p&gt;
&lt;h3&gt;Tipologie di Architettura del Kernel&lt;/h3&gt;
&lt;p&gt;Ci sono diversi approcci utilizzati per la progettazione di un kernel:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kernel Monolitico:&lt;/strong&gt; tutte le funzioni del sistema risiedono in un &lt;strong&gt;unico spazio di indirizzamento&lt;/strong&gt;. Questo approccio (usato da &lt;strong&gt;Unix&lt;/strong&gt; e &lt;strong&gt;Linux&lt;/strong&gt;) offre prestazioni elevate grazie alla comunicazione interna veloce, anche se può risultare più difficile da estendere. Molti kernel monolitici moderni sono comunque &lt;strong&gt;modulari&lt;/strong&gt;, permettendo di caricare funzionalità aggiuntive dinamicamente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Microkernel&lt;/strong&gt; (es. &lt;strong&gt;Mach&lt;/strong&gt;): fornisce solo le funzioni minime necessarie (come la comunicazione tra processi). La maggior parte dei servizi del sistema operativo viene spostata nello spazio utente per aumentare la modularità e la flessibilità.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sistemi Ibridi:&lt;/strong&gt; come &lt;strong&gt;Darwin&lt;/strong&gt; (il cuore di macOS e iOS), che combina la struttura a microkernel di Mach con parti del kernel BSD Unix.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Esempi Specifici&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; un kernel monolitico ampiamente personalizzabile. Progetti come &lt;em&gt;Linux From Scratch&lt;/em&gt; mostrano come compilare il proprio kernel partendo dal codice sorgente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Android:&lt;/strong&gt; utilizza un kernel Linux, ma aggiunge uno strato chiamato &lt;strong&gt;HAL&lt;/strong&gt; (Hardware Abstraction Layer) per poter girare su una vasta gamma di dispositivi hardware diversi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Darwin:&lt;/strong&gt; il kernel alla base dei sistemi Apple, rilasciato come software open-source.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Generazione, Avvio e Debugging&lt;/h2&gt;
&lt;p&gt;Il processo di avvio (&lt;strong&gt;boot&lt;/strong&gt;) segue solitamente questi passaggi:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Un &lt;strong&gt;programma di bootstrap&lt;/strong&gt; (nel firmware BIOS o UEFI) individua e carica il kernel.&lt;/li&gt;
&lt;li&gt;Un bootloader più evoluto, come &lt;strong&gt;GRUB&lt;/strong&gt;, può gestire il caricamento di diversi kernel o parametri di avvio.&lt;/li&gt;
&lt;li&gt;Il kernel inizializza l&#39;hardware e monta il root file system.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Il &lt;strong&gt;debugging&lt;/strong&gt; è l&#39;attività volta a risolvere i &lt;strong&gt;bug&lt;/strong&gt; e ottimizzare le prestazioni.&lt;br&gt;
I sistemi operativi facilitano questo compito tramite:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;file di log&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;core dump&lt;/strong&gt; (immagini della memoria dei processi falliti)&lt;/li&gt;
&lt;li&gt;strumenti di &lt;strong&gt;tracing&lt;/strong&gt; e &lt;strong&gt;contatori&lt;/strong&gt; per monitorare gli eventi e le chiamate di sistema&lt;/li&gt;
&lt;/ul&gt;
]]></content>
    
    
      
    
      
    <category term="sistemi operativi"/>
      
    
      
    <category term="struttura"/>
      
    
  </entry>
  
  <entry>
    <title>Processi</title>
    <link href="https://dino-996.github.io/blog/processi/" rel="alternate" type="text/html"/>
    <id>https://dino-996.github.io/blog/processi/</id>
    <published>2020-02-10T00:00:00.000Z</published>
    <updated>2020-02-10T00:00:00.000Z</updated>
    
    <summary>Vengono esaminati i meccanismi di gestione della memoria, illustrando la differenza tra l&#39;allocazione dinamica nello heap e l&#39;utilizzo temporaneo dello stack per le chiamate di funzione. Process Control Block (PCB), una struttura dati fondamentale che memorizza lo stato, i registri e le informazioni di scheduling necessarie alla CPU.</summary>
    
    
    <content type="html"><![CDATA[&lt;h1&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;In questo articolo approfondiamo l&#39;architettura dei processi all&#39;interno dei sistemi operativi, distinguendo tra il concetto di programma statico e quello di entità attiva in esecuzione.&lt;/p&gt;
&lt;h2&gt;Che cos&#39;è un processo?&lt;/h2&gt;
&lt;p&gt;Un &lt;strong&gt;processo&lt;/strong&gt; è definito come un &lt;strong&gt;programma in esecuzione&lt;/strong&gt;: un’entità attiva dotata di un contatore di programma e di un insieme di risorse associate. Si distingue dal programma, che è invece un’entità passiva, ad esempio un file memorizzato su disco.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;memoria&lt;/strong&gt;, un processo è suddiviso in sezioni specifiche:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Testo (Text Segment)&lt;/strong&gt;: contiene il codice eseguibile.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dati (Data Segment)&lt;/strong&gt;: contiene le variabili globali e statiche.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Heap&lt;/strong&gt;: area di memoria allocata dinamicamente durante l’esecuzione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stack&lt;/strong&gt;: memoria temporanea utilizzata per le chiamate di funzione e le variabili locali.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nei sistemi moderni il concetto si estende ai &lt;strong&gt;&lt;a href=&quot;https://dino-996.github.io/blog/thread/&quot;&gt;thread&lt;/a&gt;.&lt;/strong&gt;, che consentono a un singolo processo di avere più percorsi di esecuzione concorrenti, facilitando il parallelismo nei sistemi multicore.&lt;/p&gt;
&lt;h2&gt;Il core e il multicore&lt;/h2&gt;
&lt;p&gt;Un &lt;strong&gt;core&lt;/strong&gt; è un’unità di calcolo fisica indipendente all’interno della CPU.&lt;/p&gt;
&lt;p&gt;Una CPU contiene tipicamente:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Registri&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ALU (Arithmetic Logic Unit)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logica di controllo&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Capacità di eseguire un &lt;strong&gt;flusso di istruzioni indipendente&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fino ai primi anni 2000, le CPU disponevano generalmente di un solo core. Ciò significava che poteva essere eseguita una sola istruzione alla volta per ciascun ciclo di clock.&lt;/p&gt;
&lt;p&gt;Il sistema operativo utilizzava il &lt;strong&gt;time slicing&lt;/strong&gt;, una tecnica che assegna a ciascun processo un intervallo di tempo limitato di utilizzo della CPU (time slice). Ad esempio:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Processo A per 5 ms&lt;/li&gt;
&lt;li&gt;Processo B per 5 ms&lt;/li&gt;
&lt;li&gt;Processo C per 5 ms&lt;/li&gt;
&lt;li&gt;Ritorno al processo A&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Il cambio tra processi (context switch) avviene così rapidamente da risultare impercettibile all’utente. Tuttavia, il core esegue comunque una sola istruzione per volta: il parallelismo è simulato.&lt;/p&gt;
&lt;p&gt;In un sistema &lt;strong&gt;multicore&lt;/strong&gt;, invece, esistono più core fisici. Di conseguenza:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Il core 1 può eseguire il thread A&lt;/li&gt;
&lt;li&gt;Il core 2 può eseguire il thread B&lt;/li&gt;
&lt;li&gt;Il core 3 può eseguire il thread C&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tutto nello stesso istante fisico. In questo caso il &lt;strong&gt;parallelismo è reale&lt;/strong&gt;, non simulato.&lt;/p&gt;
&lt;p&gt;Un processo può avere più thread. Ogni thread:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;È un percorso di esecuzione indipendente&lt;/li&gt;
&lt;li&gt;Possiede un proprio program counter&lt;/li&gt;
&lt;li&gt;Ha un proprio stack&lt;/li&gt;
&lt;li&gt;Condivide l’heap e le risorse del processo con gli altri thread&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ad esempio, con un processo composto da 4 thread e una CPU a 4 core, il sistema operativo può assegnare un thread a ciascun core, eseguendo il programma in parallelo su più unità fisiche.&lt;/p&gt;
&lt;h2&gt;Stato e controllo del processo&lt;/h2&gt;
&lt;p&gt;Ogni processo attraversa diversi stati durante il suo ciclo di vita: &lt;strong&gt;nuovo&lt;/strong&gt;, &lt;strong&gt;pronto&lt;/strong&gt;, &lt;strong&gt;esecuzione&lt;/strong&gt;, &lt;strong&gt;attesa&lt;/strong&gt; e &lt;strong&gt;terminato&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Il sistema operativo rappresenta ogni processo tramite un &lt;strong&gt;PCB (Process Control Block)&lt;/strong&gt;, che memorizza informazioni cruciali quali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stato corrente&lt;/li&gt;
&lt;li&gt;PID (Process Identifier)&lt;/li&gt;
&lt;li&gt;Contatore di programma&lt;/li&gt;
&lt;li&gt;Registri della CPU&lt;/li&gt;
&lt;li&gt;Informazioni di memoria&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Il ciclo di vita di un processo&lt;/h2&gt;
&lt;p&gt;Un processo può trovarsi nei seguenti stati:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nuovo (New)&lt;/strong&gt;: è stato creato ma non è ancora pronto per l’esecuzione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pronto (Ready)&lt;/strong&gt;: è in memoria e attende l’assegnazione della CPU.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Esecuzione (Running)&lt;/strong&gt;: sta utilizzando la CPU.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attesa (Waiting/Blocked)&lt;/strong&gt;: è in attesa di un evento esterno, tipicamente un’operazione di I/O.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terminato (Terminated)&lt;/strong&gt;: ha completato la propria esecuzione.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Questi stati determinano in quale struttura dati del kernel il processo viene inserito e quali operazioni possono essere effettuate su di esso.&lt;/p&gt;
&lt;h2&gt;Code di scheduling&lt;/h2&gt;
&lt;p&gt;Il sistema operativo mantiene diverse code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ready Queue&lt;/strong&gt;: contiene i processi pronti all’esecuzione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Waiting Queue&lt;/strong&gt;: contiene i processi in attesa di eventi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Device Queue&lt;/strong&gt;: una coda per ciascun dispositivo di I/O.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Quando un processo richiede un’operazione di I/O:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Viene spostato nella coda del dispositivo.&lt;/li&gt;
&lt;li&gt;Il kernel assegna la CPU a un altro processo pronto.&lt;/li&gt;
&lt;li&gt;Al termine dell’I/O (tramite interrupt), il processo torna nella ready queue.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Il Process Control Block (PCB)&lt;/h2&gt;
&lt;p&gt;Il &lt;strong&gt;PCB&lt;/strong&gt; è la struttura dati con cui il kernel rappresenta un processo.&lt;/p&gt;
&lt;p&gt;Contiene:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stato del processo&lt;/li&gt;
&lt;li&gt;PID&lt;/li&gt;
&lt;li&gt;Program counter&lt;/li&gt;
&lt;li&gt;Registri della CPU&lt;/li&gt;
&lt;li&gt;Informazioni di scheduling (priorità, puntatori alle code)&lt;/li&gt;
&lt;li&gt;Informazioni di memoria (tabelle delle pagine, limiti)&lt;/li&gt;
&lt;li&gt;File aperti e risorse allocate&lt;/li&gt;
&lt;li&gt;Informazioni di accounting (tempo di CPU utilizzato)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Il PCB risiede nello spazio kernel ed è accessibile esclusivamente al sistema operativo.&lt;/p&gt;
&lt;h2&gt;Context switch&lt;/h2&gt;
&lt;p&gt;Il &lt;strong&gt;context switch&lt;/strong&gt; è il meccanismo con cui il sistema operativo interrompe un processo e ne attiva un altro.&lt;/p&gt;
&lt;p&gt;Le operazioni principali sono:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Salvare registri e program counter nel PCB del processo corrente.&lt;/li&gt;
&lt;li&gt;Selezionare un processo dalla ready queue.&lt;/li&gt;
&lt;li&gt;Ripristinare lo stato del nuovo processo dal suo PCB.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Il context switch rappresenta un overhead inevitabile del multitasking.&lt;/p&gt;
&lt;h2&gt;Scheduling e politiche decisionali&lt;/h2&gt;
&lt;p&gt;La scelta del processo successivo dipende dall’algoritmo di scheduling:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FCFS (First Come, First Served)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SJF (Shortest Job First)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Round Robin&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Priority Scheduling&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ogni algoritmo ottimizza metriche diverse:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tempo medio di attesa&lt;/li&gt;
&lt;li&gt;Tempo di risposta&lt;/li&gt;
&lt;li&gt;Throughput&lt;/li&gt;
&lt;li&gt;Equità&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Non esiste una soluzione universalmente ottimale: la scelta dipende dal contesto applicativo.&lt;/p&gt;
&lt;h2&gt;Stati estesi&lt;/h2&gt;
&lt;p&gt;Nei sistemi operativi moderni esistono stati aggiuntivi.&lt;/p&gt;
&lt;h3&gt;Suspended&lt;/h3&gt;
&lt;p&gt;Indica che il processo è temporaneamente escluso dalla competizione per la CPU.&lt;/p&gt;
&lt;p&gt;Può accadere per:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Swapping su disco&lt;/li&gt;
&lt;li&gt;Sospensione amministrativa&lt;/li&gt;
&lt;li&gt;Politiche di gestione delle risorse&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Differenza fondamentale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Waiting&lt;/strong&gt;: attesa di un evento.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Suspended&lt;/strong&gt;: esclusione per decisione del sistema o gestione della memoria.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Zombie&lt;/h3&gt;
&lt;p&gt;Un processo &lt;strong&gt;zombie&lt;/strong&gt; ha terminato l’esecuzione ma il suo PCB non è ancora stato rimosso.&lt;/p&gt;
&lt;p&gt;Il processo:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rilascia le risorse.&lt;/li&gt;
&lt;li&gt;Conserva nel PCB il codice di uscita.&lt;/li&gt;
&lt;li&gt;Attende che il padre invochi &lt;code&gt;wait()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Non consuma CPU, ma occupa una voce nella tabella dei processi.&lt;/p&gt;
&lt;h3&gt;Orphan&lt;/h3&gt;
&lt;p&gt;Un processo &lt;strong&gt;orfano&lt;/strong&gt; è un processo il cui padre termina prima di lui.&lt;/p&gt;
&lt;p&gt;Nei sistemi Unix-like viene adottato dal processo &lt;code&gt;init&lt;/code&gt; (PID 1) o da un suo equivalente, che si occuperà di raccoglierne lo stato finale.&lt;/p&gt;
&lt;h2&gt;Coordinamento nei sistemi multicore&lt;/h2&gt;
&lt;p&gt;Nei sistemi multicore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Più CPU eseguono in parallelo.&lt;/li&gt;
&lt;li&gt;Le strutture dati del kernel sono condivise.&lt;/li&gt;
&lt;li&gt;Le decisioni di scheduling possono avvenire simultaneamente.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Strutture condivise&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ready queue&lt;/li&gt;
&lt;li&gt;Tabelle dei processi (PCB)&lt;/li&gt;
&lt;li&gt;Tabelle delle pagine&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Accessi concorrenti possono causare condizioni di race.&lt;/p&gt;
&lt;h3&gt;Meccanismi di sincronizzazione&lt;/h3&gt;
&lt;p&gt;Il kernel utilizza:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spinlock&lt;/li&gt;
&lt;li&gt;Mutex&lt;/li&gt;
&lt;li&gt;Sezioni critiche&lt;/li&gt;
&lt;li&gt;Disabilitazione temporanea degli interrupt&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;per garantire coerenza.&lt;/p&gt;
&lt;h3&gt;Affinità della CPU&lt;/h3&gt;
&lt;p&gt;La &lt;strong&gt;CPU affinity&lt;/strong&gt; consente di associare preferenzialmente un processo a un core specifico per:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Migliorare la località della cache&lt;/li&gt;
&lt;li&gt;Ridurre il costo di migrazione tra core&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La migrazione tra core comporta invalidazioni di cache e costi aggiuntivi.&lt;/p&gt;
&lt;h2&gt;Comunicazione tra processi (IPC)&lt;/h2&gt;
&lt;p&gt;I processi cooperanti utilizzano meccanismi di &lt;strong&gt;IPC (InterProcess Communication)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Due modelli principali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memoria condivisa&lt;/strong&gt;: comunicazione tramite una regione di memoria comune; è veloce ma richiede sincronizzazione.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scambio di messaggi&lt;/strong&gt;: basato su primitive &lt;code&gt;send()&lt;/code&gt; e &lt;code&gt;receive()&lt;/code&gt;, può essere sincrono (bloccante) o asincrono (non bloccante).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Esempi di sistemi IPC e comunicazione client-server&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pipe&lt;/strong&gt;: canali di comunicazione, tipicamente tra processi correlati.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Socket&lt;/strong&gt;: identificati da indirizzo IP e porta; fondamentali per la comunicazione in rete (TCP e UDP).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RPC (Remote Procedure Call)&lt;/strong&gt;: permettono di invocare procedure remote come se fossero locali.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sistemi specifici&lt;/strong&gt;: Mach utilizza porte e messaggi; Windows impiega le &lt;strong&gt;ALPC (Advanced Local Procedure Call)&lt;/strong&gt; per la comunicazione locale.&lt;/li&gt;
&lt;/ul&gt;
]]></content>
    
    
      
    
      
    <category term="sistemi operativi"/>
      
    
      
    <category term="processi"/>
      
    
  </entry>
  
  <entry>
    <title>Sincronizzazione dei processi</title>
    <link href="https://dino-996.github.io/blog/sincronizzazione-dei-processi/" rel="alternate" type="text/html"/>
    <id>https://dino-996.github.io/blog/sincronizzazione-dei-processi/</id>
    <published>2020-02-03T00:00:00.000Z</published>
    <updated>2020-02-03T00:00:00.000Z</updated>
    
    <summary>Viene esaminata la sincronizzazione dei processi nei sistemi operativi, focalizzandosi sulla gestione dell&#39;accesso concorrente a dati condivisi per prevenire le race condition.</summary>
    
    
    <content type="html"><![CDATA[&lt;h1&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;In questo articolo esploriamo la sincronizzazione dei processi per garantire mutua esclusione, progresso e attesa limitata nella sezione critica. Vengono illustrati strumenti che spaziano dal supporto hardware (CAS) a mutex, semafori e monitor, analizzando criticità di liveness come stallo e inversione di priorità. La selezione della tecnica più efficiente viene infine valutata in base al livello di contesa del sistema.&lt;/p&gt;
&lt;h2&gt;Sincronizzazione dei Processi&lt;/h2&gt;
&lt;p&gt;La sincronizzazione dei processi è uno dei problemi fondamentali dell’informatica dei sistemi. Quando più thread o processi accedono a dati condivisi, il rischio di inconsistenza diventa concreto. Comprendere e progettare correttamente la &lt;strong&gt;sezione critica&lt;/strong&gt; è essenziale per garantire sicurezza, affidabilità e scalabilità.&lt;/p&gt;
&lt;h2&gt;Il Problema della Sezione Critica&lt;/h2&gt;
&lt;p&gt;Una &lt;strong&gt;sezione critica (Critical Section, CS)&lt;/strong&gt; è la porzione di codice in cui un &lt;a href=&quot;https://dino-996.github.io/blog/processi/&quot;&gt;processo&lt;/a&gt; o &lt;a href=&quot;https://dino-996.github.io/blog/thread/&quot;&gt;thread&lt;/a&gt; accede e modifica risorse condivise.&lt;/p&gt;
&lt;p&gt;Qualsiasi soluzione corretta deve soddisfare tre proprietà fondamentali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mutua esclusione&lt;/strong&gt;: se un processo è nella CS, nessun altro può entrarvi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Progresso&lt;/strong&gt;: la scelta del prossimo processo da far entrare non può essere rinviata indefinitamente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attesa limitata&lt;/strong&gt;: esiste un limite massimo al numero di accessi consentiti ad altri processi prima che una richiesta venga soddisfatta.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Kernel preemptive e non preemptive&lt;/h3&gt;
&lt;p&gt;Nei kernel &lt;strong&gt;non preemptive&lt;/strong&gt;, un processo in modalità kernel non viene interrotto fino al termine della sua esecuzione in tale modalità. Questo riduce il rischio di race condition interne al kernel, ma non costituisce una protezione generale: le condizioni di gara possono comunque verificarsi tra thread in user space o in sistemi multiprocessore.&lt;/p&gt;
&lt;p&gt;Nei sistemi moderni, prevalentemente &lt;strong&gt;preemptive e multi-core&lt;/strong&gt;, la sincronizzazione è un requisito strutturale.&lt;/p&gt;
&lt;h2&gt;Modelli di Memoria e Riordino delle Istruzioni&lt;/h2&gt;
&lt;p&gt;Le architetture moderne (x86, ARM, Power) adottano &lt;strong&gt;modelli di memoria debolmente ordinati&lt;/strong&gt;. Compilatori e CPU possono riordinare le istruzioni per ottimizzazione.&lt;/p&gt;
&lt;p&gt;Senza adeguate &lt;strong&gt;memory barrier&lt;/strong&gt; o &lt;strong&gt;memory fence&lt;/strong&gt; (istruzioni che impongono un vincolo sull’ordine con cui operazioni di lettura e scrittura in memoria possono essere eseguite e rese visibili agli altri thread), un algoritmo teoricamente corretto può fallire nella pratica.&lt;/p&gt;
&lt;p&gt;Questo è il motivo per cui soluzioni puramente software come l’algoritmo di Peterson non sono affidabili su hardware moderno senza primitive di sincronizzazione esplicite.&lt;/p&gt;
&lt;h2&gt;Soluzioni Software e Supporto Hardware&lt;/h2&gt;
&lt;h3&gt;Algoritmo di Peterson&lt;/h3&gt;
&lt;p&gt;Soluzione elegante per due processi basata su variabili condivise (&lt;code&gt;flag&lt;/code&gt; e &lt;code&gt;turn&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Limiti principali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non scala oltre due thread.&lt;/li&gt;
&lt;li&gt;Non sicura su architetture moderne senza memory fence.&lt;/li&gt;
&lt;li&gt;Non adatta a sistemi ad alta contesa.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;È oggi più rilevante come esercizio teorico che come soluzione pratica.&lt;/p&gt;
&lt;h3&gt;Primitive Atomiche Hardware&lt;/h3&gt;
&lt;p&gt;Le CPU moderne offrono istruzioni atomiche come:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;test_and_set&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compare_and_swap (CAS)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Il &lt;strong&gt;CAS&lt;/strong&gt; è particolarmente importante nei sistemi concorrenti moderni.&lt;/p&gt;
&lt;p&gt;Esempio concettuale:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CAS(address, expected, new_value)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aggiorna il valore solo se coincide con &lt;code&gt;expected&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Problemi possibili:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ABA problem&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Starvation&lt;/strong&gt; in presenza di alta contesa&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Queste primitive sono alla base di strutture dati &lt;strong&gt;lock-free&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Strumenti di Sincronizzazione di Alto Livello&lt;/h2&gt;
&lt;h3&gt;Mutex&lt;/h3&gt;
&lt;p&gt;Strumento basilare per garantire mutua esclusione.&lt;/p&gt;
&lt;p&gt;Due categorie principali:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Spinlock&lt;/strong&gt;: attesa attiva (busy waiting). Efficiente per sezioni critiche molto brevi su sistemi multi-core.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mutex bloccanti&lt;/strong&gt;: sospendono il thread, evitando spreco di CPU ma con overhead di context switch.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Semafori&lt;/h3&gt;
&lt;p&gt;Variabili intere gestite tramite operazioni atomiche:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;wait()&lt;/code&gt; (P)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;signal()&lt;/code&gt; (V)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Possono essere:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Binari&lt;/strong&gt; (equivalenti a mutex)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contatori&lt;/strong&gt; (gestione pool di risorse)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Monitor&lt;/h3&gt;
&lt;p&gt;Costrutti di alto livello che incapsulano:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dati condivisi&lt;/li&gt;
&lt;li&gt;Procedure&lt;/li&gt;
&lt;li&gt;Variabili condizionali&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Garantiscono mutua esclusione automatica.&lt;/p&gt;
&lt;h2&gt;Esempi in Python e Java&lt;/h2&gt;
&lt;h3&gt;Python (threading)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import threading

lock = threading.Lock()
counter = 0

def increment():
    global counter
    with lock:
        counter += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nota: a causa del &lt;strong&gt;GIL (Global Interpreter Lock)&lt;/strong&gt;, il threading in CPython non è realmente parallelo su CPU-bound tasks.&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;Java&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import java.util.concurrent.atomic.AtomicInteger;

AtomicInteger counter = new AtomicInteger(0);

public void increment() {
    counter.incrementAndGet();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Java fornisce:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;synchronized&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ReentrantLock&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;package &lt;code&gt;java.util.concurrent&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Il modello di memoria Java definisce formalmente la visibilità delle variabili condivise.&lt;/p&gt;
&lt;h2&gt;Liveness e Problemi Classici&lt;/h2&gt;
&lt;p&gt;La &lt;strong&gt;liveness&lt;/strong&gt; misura la capacità del sistema di garantire progresso.&lt;/p&gt;
&lt;p&gt;Principali criticità:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Deadlock&lt;/strong&gt;: attesa circolare indefinita.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Starvation&lt;/strong&gt;: un processo non ottiene mai accesso alla risorsa.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Priority Inversion&lt;/strong&gt;: un processo ad alta priorità bloccato da uno a bassa priorità.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Caso emblematico: &lt;strong&gt;Mars Pathfinder (1997)&lt;/strong&gt;. Il problema fu risolto abilitando il meccanismo di &lt;em&gt;priority inheritance&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Sincronizzazione e Scalabilità in Ambiente Cloud Native&lt;/h2&gt;
&lt;p&gt;Nei &lt;strong&gt;sistemi distribuiti&lt;/strong&gt; (insieme di computer indipendenti che collaborano tra loro come se fossero un unico sistema coerente agli occhi dell’utente o delle applicazioni), la sincronizzazione non riguarda più solo thread locali.&lt;/p&gt;
&lt;p&gt;Si utilizzano:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lock distribuiti&lt;/strong&gt; (meccanismo che garantisce mutua esclusione tra processi o servizi che girano su nodi diversi di un sistema distribuito) (&lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt;, &lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;ZooKeeper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transazioni database&lt;/strong&gt; (insieme di operazioni che vengono trattate con un&#39;unica unità logica di lavoro)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Algoritmi di consenso&lt;/strong&gt; (meccanismo che permette a un insieme di nodi di un sistema distribuito di accordarsi su un singolo valore o decisione, anche in presenza di guasti o comunicazioni ritardate) (&lt;a href=&quot;https://raft.github.io/raft.pdf&quot;&gt;Raft&lt;/a&gt;, &lt;a href=&quot;https://www.cs.cornell.edu/home/rvr/Paxos/paxos.pdf&quot;&gt;Paxos&lt;/a&gt;) - Per approfondire: &lt;a href=&quot;https://arxiv.org/abs/2004.05074&quot;&gt;Paxos vs Raft: Have we reached consensus on distributed consensus?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In architetture &lt;strong&gt;microservizi&lt;/strong&gt;, l’eccesso di lock distribuiti può compromettere la scalabilità (intesa come capacità di un sistema di gestire un aumento del carico di lavoro senza degradare significativamente le prestazioni.).&lt;/p&gt;
&lt;p&gt;Approcci moderni:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design &lt;strong&gt;event-driven&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Code di messaggi&lt;/li&gt;
&lt;li&gt;Architetture idempotenti&lt;/li&gt;
&lt;li&gt;Programmazione lock-free&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Valutazione in Base al Livello di Contesa&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bassa o moderata contesa&lt;/strong&gt;: strutture basate su CAS spesso più performanti.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Alta contesa&lt;/strong&gt;: mutex e meccanismi bloccanti possono risultare più efficienti.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La scelta dipende da:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Durata della sezione critica&lt;/li&gt;
&lt;li&gt;Numero di core&lt;/li&gt;
&lt;li&gt;Pattern di accesso&lt;/li&gt;
&lt;li&gt;Requisiti di latenza&lt;/li&gt;
&lt;/ul&gt;
]]></content>
    
    
      
    
      
    <category term="sistemi operativi"/>
      
    
      
    <category term="processi"/>
      
    
  </entry>
  
  <entry>
    <title>Stallo dei processi</title>
    <link href="https://dino-996.github.io/blog/stallo-dei-processi/" rel="alternate" type="text/html"/>
    <id>https://dino-996.github.io/blog/stallo-dei-processi/</id>
    <published>2020-02-01T00:00:00.000Z</published>
    <updated>2020-02-01T00:00:00.000Z</updated>
    
    <summary>Viene esaminato il fenomeno dello stallo dei processi (deadlock) nei sistemi operativi, analizzando come più thread competano per risorse limitate.</summary>
    
    
    <content type="html"><![CDATA[&lt;h1&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;In questo articolo approfondiamo le quattro condizioni necessarie affinché si verifichi un blocco totale, distinguendo tra il classico deadlock e il livelock, o stallo attivo. Vengono illustrati diversi strumenti diagnostici, come il grafo di assegnazione delle risorse, e algoritmi complessi come quello del banchiere per garantire uno stato di esecuzione sicuro. Infine, vengono descritte le tecniche di ripristino, che includono la terminazione forzata dei processi o la prelazione delle risorse per interrompere i cicli di attesa circolare.&lt;/p&gt;
&lt;h2&gt;Deadlock&lt;/h2&gt;
&lt;p&gt;Lo &lt;strong&gt;stallo (deadlock)&lt;/strong&gt; è una condizione in cui un insieme di thread o processi rimane bloccato perché ciascuno attende una risorsa detenuta da un altro elemento del gruppo.&lt;/p&gt;
&lt;p&gt;In un sistema con multiprogrammazione, le entità concorrenti competono per un numero finito di &lt;strong&gt;risorse&lt;/strong&gt; (CPU, memoria, file, socket, lock, connessioni database). Una risorsa viene:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Richiesta&lt;/li&gt;
&lt;li&gt;Assegnata&lt;/li&gt;
&lt;li&gt;Utilizzata&lt;/li&gt;
&lt;li&gt;Rilasciata&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Se una risorsa è già assegnata, il richiedente entra in stato di attesa. In determinate condizioni, questa attesa può diventare permanente.&lt;/p&gt;
&lt;h2&gt;Le quattro condizioni necessarie allo stallo&lt;/h2&gt;
&lt;p&gt;Affinché si verifichi un deadlock, devono coesistere simultaneamente quattro condizioni (Coffman conditions):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Mutua esclusione&lt;/strong&gt;
Almeno una risorsa deve essere non condivisibile (es. un lock esclusivo).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Possesso e attesa (Hold and Wait)&lt;/strong&gt;
Un thread detiene almeno una risorsa e ne attende altre.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assenza di prelazione (No Preemption)&lt;/strong&gt;
Le risorse non possono essere sottratte forzatamente; devono essere rilasciate volontariamente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attesa circolare (Circular Wait)&lt;/strong&gt;
Esiste una catena chiusa di attese tra thread/processi.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Rimuovere anche solo una di queste condizioni impedisce il verificarsi dello stallo.&lt;/p&gt;
&lt;h2&gt;Deadlock, Livelock e Starvation&lt;/h2&gt;
&lt;p&gt;È essenziale distinguere tre fenomeni diversi:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Deadlock&lt;/strong&gt;: blocco permanente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Livelock&lt;/strong&gt;: i thread sono attivi ma non progrediscono (ad esempio, rilasciano e riacquisiscono risorse continuamente).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Starvation&lt;/strong&gt;: un thread non ottiene mai una risorsa a causa di politiche di scheduling sbilanciate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In sistemi ad alta concorrenza, la starvation è spesso più subdola del deadlock.&lt;/p&gt;
&lt;h2&gt;Grafo di Assegnazione delle Risorse (RAG)&lt;/h2&gt;
&lt;p&gt;Le situazioni di stallo possono essere modellate tramite un &lt;strong&gt;Resource Allocation Graph&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nodi circolari → thread/processi&lt;/li&gt;
&lt;li&gt;Nodi rettangolari → risorse&lt;/li&gt;
&lt;li&gt;Arco da thread a risorsa → richiesta&lt;/li&gt;
&lt;li&gt;Arco da risorsa a thread → assegnazione&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Proprietà fondamentali&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Nessun ciclo → nessun deadlock&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ciclo con una sola istanza per risorsa → deadlock certo&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ciclo con più istanze → condizione necessaria ma non sufficiente&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nel caso di risorse con singola istanza, si utilizza spesso il &lt;strong&gt;grafo di attesa&lt;/strong&gt;, più semplice da analizzare.&lt;/p&gt;
&lt;h2&gt;Strategie di Gestione dello Stallo&lt;/h2&gt;
&lt;h3&gt;Ignorare il problema (Approccio “Ostrich”)&lt;/h3&gt;
&lt;p&gt;Sistemi come Microsoft Windows e Linux non implementano meccanismi generali di prevenzione del deadlock a livello kernel per tutte le risorse applicative.&lt;/p&gt;
&lt;p&gt;La responsabilità è delegata allo sviluppatore. È una scelta pragmatica: prevenzione ed evitamento hanno costi computazionali elevati.&lt;/p&gt;
&lt;h3&gt;Prevenzione&lt;/h3&gt;
&lt;p&gt;Consiste nell’eliminare una delle quattro condizioni necessarie. Un esempio pratico può essere quello di prevenire l’attesa circolare imponendo un &lt;strong&gt;ordinamento globale dei lock&lt;/strong&gt;. Se ogni thread acquisisce sempre le risorse nello stesso ordine, il ciclo non può formarsi.&lt;/p&gt;
&lt;h3&gt;Evitamento&lt;/h3&gt;
&lt;p&gt;Richiede conoscenza preventiva delle richieste massime di risorse. L’&lt;strong&gt;algoritmo del banchiere&lt;/strong&gt; verifica che ogni assegnazione mantenga il sistema in uno &lt;strong&gt;stato sicuro&lt;/strong&gt;, ossia uno stato da cui esiste almeno una sequenza di completamento per tutti i processi. È elegante dal punto di vista teorico, ma raramente applicato nei sistemi general-purpose moderni a causa dell’overhead e della necessità di informazioni complete.&lt;/p&gt;
&lt;h4&gt;Algoritmo del banchiere&lt;/h4&gt;
&lt;p&gt;La complessità di questo algoritmo è $O(n^2 m)$, dove $n$ è il numero di processi e $m$ il numero di tipi di risorse.&lt;/p&gt;
&lt;p&gt;Supponiamo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 processi: $P_0, P_1, P_2$&lt;/li&gt;
&lt;li&gt;3 tipi di risorse: $A, B, C$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Risorse totali disponibili: $&#92;text{Avaible} = [3,3,2]$&lt;/p&gt;
&lt;h5&gt;Matrice delle richieste massime:&lt;/h5&gt;
&lt;div class=&quot;mb-2&quot;&gt;
$
&#92;text{Max}=
&#92;begin{bmatrix}
7 &amp; 5 &amp; 3 &#92;&#92;
3 &amp; 2 &amp; 2 &#92;&#92;
9 &amp; 0 &amp; 2
&#92;end{bmatrix}
$
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;indica il &lt;strong&gt;massimo&lt;/strong&gt; che ogni processo potrebbe richiedere per ciascuna risorsa. Ad esempio, $P_0$ potrebbe richiedere al massimo 7 unità di $A$, 5 di $B$ e 3 di $C$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Risorse già assegnate:&lt;/h5&gt;
&lt;div class=&quot;mb-2&quot;&gt;
$
&#92;text{Allocation}=
&#92;begin{bmatrix}
0 &amp; 1 &amp; 0 &#92;&#92;
2 &amp; 0 &amp; 0 &#92;&#92;
3 &amp; 0 &amp; 2
&#92;end{bmatrix}
$
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;indica quante risorse sono &lt;strong&gt;già state date&lt;/strong&gt; a ciascun processo. Ad esempio, $P_1$ ha già ricevuto 2 unità di $A$, 0 di $B$ e 0 di $C$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Calcolo delle risorse ancora necessarie:&lt;/h5&gt;
&lt;div class=&quot;mb-2&quot;&gt;
$
&#92;text{Need}=&#92;text{Max - Allocation}=
&#92;begin{bmatrix}
7 &amp; 4 &amp; 3 &#92;&#92;
1 &amp; 2 &amp; 2 &#92;&#92;
6 &amp; 0 &amp; 0
&#92;end{bmatrix}
$
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Si ottiene sottraendo riga per riga: ad esempio per $P_0$, $&#92;text{Need}[0] = [7,5,3] - [0,1,0] = [7,4,3]$. Rappresenta quante risorse mancano ancora a ciascun processo per completare la propria esecuzione.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Verifica dello stato sicuro&lt;/h5&gt;
&lt;p&gt;L&#39;algoritmo cerca iterativamente un processo $P_i$ che soddisfi la condizione $&#92;text{Need}[i] &#92;leq &#92;text{Avaible}$, ovvero un processo che possa essere eseguito con le risorse attualmente disponibili. Se lo trova, simula la sua esecuzione: le risorse che gli erano state assegnate vengono &amp;quot;liberate&amp;quot; e aggiunte ad $&#92;text{Avaible}$. Si ripete finché tutti i processi sono stati eseguiti (stato sicuro) oppure non si trova più nessun processo eseguibile (stato non sicuro).&lt;/p&gt;
&lt;p&gt;Partendo da $P_0$ con $&#92;text{Avaible} = [3,3,2]$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P_0 &#92;text{: } &#92;text{Need[0]} = [7,4,3] &#92;to &#92;text{Non soddisfa le condizioni: } 7 &amp;gt; 3 &#92;text{ già alla prima risorsa.}$&lt;/li&gt;
&lt;li&gt;$P_1 &#92;text{: } &#92;text{Need[1]} = [1,2,2] &#92;to &#92;text{Soddisfa le condizioni: } 1 &#92;leq 3, 2 &#92;leq 3, 2 &#92;leq 2. $&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$P_1$ viene eseguito e le risorse vengono rilasciate. $&#92;text{Avaible}$ si aggiorna sommando $&#92;text{Allocation}[1] = [2,0,0]$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$&#92;text{Avaible} = [3,3,2] + [2,0,0] = [5,3,2]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ripartiamo da $P_0$ con $&#92;text{Avaible} = [5,3,2]$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P_0 &#92;text{: } &#92;text{Need[0]} = [7,4,3] &#92;to &#92;text{Non soddisfa le condizioni: } 7 &amp;gt; 5 &#92;text{ già alla prima risorsa.}$&lt;/li&gt;
&lt;li&gt;$P_2 &#92;text{: } &#92;text{Need[2]} = [6,0,0] &#92;to &#92;text{Non soddisfa le condizioni: } 6 &amp;gt; 5 &#92;text{ già alla prima risorsa.}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nessun processo è eseguibile: non è possibile trovare una sequenza sicura. &lt;strong&gt;Lo stato è non sicuro&lt;/strong&gt;, il che significa che il sistema non può garantire che tutti i processi riescano a completarsi. L&#39;algoritmo del banchiere avrebbe quindi rifiutato l&#39;allocazione che ha portato a questo stato.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusione&lt;/strong&gt;: se provassimo a dare subito le risorse disponibili senza priorità andremmo in contro a uno stato non sicuro.&lt;/p&gt;
&lt;p&gt;Se inizialmente avremmo avuto $&#92;text{Avaible}=[10,5,7]$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P_0 &#92;text{: } &#92;text{Need[0]} = [7,4,3] &#92;to &#92;text{Soddisfa le condizioni: } 7 &#92;leq 12, 4 &#92;leq 5, 3 &#92;leq 7. &#92;quad &#92;text{Avaible} = [12,5,7] + [0,1,0] = [12,6,7]$&lt;/li&gt;
&lt;li&gt;$P_1 &#92;text{: } &#92;text{Need[1]} = [1,2,2] &#92;to &#92;text{Soddisfa le condizioni: } 1 &#92;leq 10, 2 &#92;leq 5, 2 &#92;leq 7. &#92;quad &#92;text{Avaible} = [10,5,7] + [2,0,0] = [12,5,7]$&lt;/li&gt;
&lt;li&gt;$P_2 &#92;text{: } &#92;text{Need[2]} = [6,0,0] &#92;to &#92;text{Soddisfa le condizioni: } 6 &#92;leq 12, 0 &#92;leq 6, 0 &#92;leq 7. &#92;quad &#92;text{Avaible} = [12,6,7] + [3,0,2] = [15,6,9]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sequenza sicura&lt;/strong&gt;: $P_0 &#92;to P_1 &#92;to P_2$&lt;/p&gt;
&lt;h3&gt;Rilevamento e Ripristino&lt;/h3&gt;
&lt;p&gt;Utilizzato soprattutto nei database e nei sistemi transazionali.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rilevamento periodico tramite analisi del grafo di attesa.&lt;/li&gt;
&lt;li&gt;Ripristino tramite:
&lt;ul&gt;
&lt;li&gt;Terminazione di processi&lt;/li&gt;
&lt;li&gt;Prelazione con rollback&lt;/li&gt;
&lt;li&gt;Selezione di una “vittima” secondo criteri di costo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Deadlock nella pratica: esempi concreti&lt;/h1&gt;
&lt;h2&gt;Esempio in Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import threading
import time

lock1 = threading.Lock()
lock2 = threading.Lock()

def thread1():
    with lock1:
        time.sleep(1)
        with lock2:
            print(&amp;quot;Thread 1 acquisito lock2&amp;quot;)

def thread2():
    with lock2:
        time.sleep(1)
        with lock1:
            print(&amp;quot;Thread 2 acquisito lock1&amp;quot;)

t1 = threading.Thread(target=thread1)
t2 = threading.Thread(target=thread2)

t1.start()
t2.start()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Qui:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thread 1 acquisisce &lt;code&gt;lock1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Thread 2 acquisisce &lt;code&gt;lock2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Entrambi attendono il lock detenuto dall’altro&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deadlock perfetto.&lt;/p&gt;
&lt;h3&gt;Soluzione&lt;/h3&gt;
&lt;p&gt;Imporre un ordine globale di acquisizione dei lock.&lt;/p&gt;
&lt;h2&gt;Esempio in Java&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class DeadlockExample {

    private static final Object lock1 = new Object();
    private static final Object lock2 = new Object();

    public static void main(String[] args) {

        Thread t1 = new Thread(() -&amp;gt; {
            synchronized (lock1) {
                try { 
                    Thread.sleep(100); 
                } catch (InterruptedException e) {}
                synchronized (lock2) {}
            }
        });

        Thread t2 = new Thread(() -&amp;gt; {
            synchronized (lock2) {
                try { 
                    Thread.sleep(100); 
                } catch (InterruptedException e) {}
                synchronized (lock1) {}
            }
        });

        t1.start();
        t2.start();
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In ambienti enterprise basati su Java, questo errore può compromettere interi application server.&lt;/p&gt;
&lt;h1&gt;Deadlock e Cloud Native&lt;/h1&gt;
&lt;p&gt;Nel paradigma &lt;strong&gt;Cloud Native&lt;/strong&gt;, il deadlock non riguarda solo thread locali.&lt;/p&gt;
&lt;p&gt;Può emergere tra:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Microservizi che attendono risposte reciproche&lt;/li&gt;
&lt;li&gt;Pool di connessioni database saturi&lt;/li&gt;
&lt;li&gt;Lock distribuiti&lt;/li&gt;
&lt;li&gt;Transazioni distribuite&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Un pool di connessioni esaurito può generare un deadlock applicativo che si traduce in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Saturazione CPU&lt;/li&gt;
&lt;li&gt;Timeout a cascata&lt;/li&gt;
&lt;li&gt;Riduzione della disponibilità (effetto simile a un DoS logico)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Qui il problema diventa anche di &lt;strong&gt;sicurezza e resilienza operativa&lt;/strong&gt;.&lt;/p&gt;
]]></content>
    
    
      
    
      
    <category term="sistemi operativi"/>
      
    
      
    <category term="processi"/>
      
    
  </entry>
  

</feed>